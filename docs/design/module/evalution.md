# 🔥 ARF 模块开发参考文档：数据转换模块 (Converter)

> 🎯 **角色定位:** ARF的"数据精炼厂" - 将原始数据转化为标准化的训练资产。
>
> 📦 **模块代号:** `arf-cloud-converter`
>
> ⚡ **所属:** ARF 云端平面 (Cloud Plane)

---

## 📋 1. 核心职责与设计理念

### 🎯 核心使命 (Core Mission)

作为`数据中心`和`训练模块`之间的关键桥梁，数据转换模块的核心使命是**将各种来源、各种格式的异构、原始数据，通过一系列可配置的处理步骤，转化为统一的、可供训练的结构化格式**。它是保证我们“垃圾进，黄金出”(Garbage In, Gold Out)数据质量的核心。

主要应用场景包括：

- **🧩 格式解析与转换:** 解析ROS Bag, CSV, JSON等多种数据格式，并将其转换为统一的Parquet/TFRecord格式。
- **🧹 数据清洗与增强:** 执行数据去重、异常值处理、时间戳对齐、数据增强（如图像翻转、裁剪）等操作。
- **🏷️ 自动/辅助标注:** 与主流数据标注平台（如Label Studio）集成，或运行自动标注模型，为数据生成标签。
- **🧬 特征工程:** 从原始数据中提取更有价值的特征（如从IMU数据计算姿态角）。

### 🏗️ 核心架构：“流水线即代码” (Pipeline-as-Code)

我们将复杂的数据处理流程，抽象为一系列可编排的、代码化的**“数据流水线”**。

- **原子化算子 (Atomic Operators):** 每一个数据处理步骤（如“读取rosbag”、“图像去模糊”、“写入parquet”）都被封装成一个独立的、可复用的**“算子”**（容器化程序）。
- **有向无环图 (DAG):** 开发者通过一个YAML配置文件，将这些“算子”组合成一个有向无环图（DAG），来定义一个完整的数据转换流水线。
- **工作流引擎 (Workflow Engine):** `数据转换模块`的核心是一个工作流引擎（如Argo Workflows），它负责解析DAG配置，并调度Kubernetes集群来执行这些算子容器。

### ⚖️ 设计原则 (Design Principles)

- **🧩 模块化与可复用:** 每个处理算子都应该是独立的、职责单一的，并且可以在不同的流水线中被复用。
- **⚙️ 可配置性:** 整个数据处理流程应完全由配置文件驱动，无需修改核心代码即可适应新的数据源和处理需求。
- **🚀 可伸缩性:** 架构必须能利用云的弹性，处理从几GB到几PB规模的数据。
- **🔄 可复现性:** 任何一次数据转换的结果都必须是可复现的。

---

## 📝 2. 核心需求 (Core Requirements)

| ID     | 需求描述           | 验收标准                                                     | 优先级   |
| :----- | :----------------- | :----------------------------------------------------------- | :------- |
| **C1** | **多格式输入支持** | 必须支持至少三种常见机器人数据格式的解析，如ROS Bag, CSV, aedat4（事件相机）。 | **最高** |
| **C2** | **流水线编排**     | 必须支持通过YAML文件定义和执行多步骤的数据处理流水线（DAG）。 | **最高** |
| **C3** | **与数据中心集成** | 流水线必须能够从`数据中心`读取原始数据，并将处理后的结果写回`数据中心`的一个新位置。 | **最高** |
| **C4** | **自定义算子支持** | 必须提供一个清晰的SDK和模板，让开发者可以轻松地创建和注册自定义的数据处理“算子”。 | **高**   |
| **C5** | **大规模数据处理** | 必须支持分布式数据处理框架（如Spark, Dask），以处理单机无法承载的大规模数据集。 | **中**   |

---

## ⚙️ 3. 关键功能与子模块架构

### 🧠 3.1 转换任务管理器 (Converter Job Manager)

**总调度师角色**：这是一个用`Go`实现的、运行在Kubernetes上的核心服务。

- **API端点:** 对外提供一个gRPC服务（`converter.proto`），接收开发者提交的数据转换流水线任务。
- **DAG解析器:** 解析用户提交的YAML配置文件，理解算子之间的依赖关系。
- **工作流生成器:** 将解析后的DAG，动态地翻译成一个Argo Workflows或Tekton的流水线CRD（Custom Resource Definition）。
- **任务提交与监控:** 调用Kubernetes API来创建和运行该流水线，并持续监控其执行状态。

### 🏭 3.2 算子执行器 (Operator Executor)

**工人角色**：流水线中的每一个步骤，都是一个运行在Kubernetes Pod中的**算子容器**。

- **标准算子库:** ARF官方提供一个包含常用数据处理功能的算子库，如`rosbag-reader`, `image-decoder`, `parquet-writer`等。
- **自定义算子:** 开发者可以编写自己的Python/C++程序，并将其打包成容器镜像，然后在流水线YAML中引用。
- **数据交互:** 算子之间通过挂载到Pod的共享存储卷（如PVC）或对象存储(S3)来传递数据。例如，`rosbag-reader`将解析出的图像保存为临时文件，`image-decoder`读取这些文件进行处理。

---

## 🔗 4. 接口设计与数据流

`数据转换模块`是云端数据处理流程的核心。

### 📥 输入数据流

| **数据源**      | **数据类型**                  | **优先级** | **延迟要求** | **示例场景**                   |
| :-------------- | :---------------------------- | :--------- | :----------- | :----------------------------- |
| **数据中心**    | 原始数据文件 (ROS Bag, etc.)  | `NORMAL`   | N/A (离线)   | 流水线的第一步，读取原始数据   |
| **开发者(CLI)** | `RunConversionRequest` (gRPC) | `NORMAL`   | `< 5s`       | 开发者手动提交一个数据处理任务 |

### 📤 输出数据流

| **目标模块**    | **数据类型**                  | **保证**     | **性能指标**     |
| :-------------- | :---------------------------- | :----------- | :--------------- |
| **数据中心**    | 标准化的训练数据集 (.parquet) | 可靠写入     | PB级数据处理能力 |
| **开发者(CLI)** | `JobStatus` (gRPC)            | 实时状态更新 | -                |

### 🧬 核心API草案 (`converter.proto`)

```protobuf
// protos/arf/cloud/v1/converter.proto
syntax = "proto3";

package arf.cloud.v1;

// ... import job_status ...

// 数据转换服务
service ConverterService {
  // 提交一个新的数据转换流水线任务
  rpc RunConversion(RunConversionRequest) returns (RunConversionResponse);
  // 获取一个任务的状态
  rpc GetConversionStatus(GetConversionStatusRequest) returns (arf.cloud.v1.JobStatus);
}

message RunConversionRequest {
    string display_name = 1;
    // Base64编码的流水线YAML定义文件内容
    string pipeline_yaml_base64 = 2;
    // (可选) 传入的参数
    map<string, string> parameters = 3;
}
message RunConversionResponse {
    string job_id = 1;
}

message GetConversionStatusRequest {
    string job_id = 1;
}
```

------



## 🛠️ 5. 技术栈与开发环境





### 💻 核心技术栈



| **技术领域**     | **选型**                     | **版本要求** | **用途说明**             |
| ---------------- | ---------------------------- | ------------ | ------------------------ |
| **核心服务语言** | **Go**                       | `1.21+`      | 构建高并发的任务管理器   |
| **算子开发语言** | **Python**                   | `3.10+`      | 利用丰富的数据科学生态   |
| **工作流引擎**   | **Argo Workflows / Tekton**  | 最新稳定版   | 编排和执行数据处理DAG    |
| **分布式处理**   | **Apache Spark / Dask**      | 最新稳定版   | (可选)用于大规模数据处理 |
| **数据格式**     | **Apache Parquet, TFRecord** | 最新稳定版   | 标准化的训练数据格式     |

------



## 🔧 6. 开发实施细节





### 🏗️ 6.1 项目结构 V1



```
services/cloud-plane/converter-manager/ # 转换任务管理器 (Go)
├── internal/
│   ├── scheduler/                  # Argo/Tekton任务调度逻辑
│   └── server/                     # gRPC服务器实现
├── Dockerfile
└── ...

operators/                          # 标准算子库
├── readers/
│   └── rosbag-reader/              # 一个算子的项目目录
│       ├── main.py
│       └── Dockerfile
└── writers/
    └── parquet-writer/
        └── ...
```



### 🧪 6.2 测试与验证策略



- **单元测试:** 对每一个独立的算子进行单元测试。
- **流水线集成测试:** 编写一个包含“读取->处理->写入”的简单流水线YAML，并提交给`ConverterService`执行，验证整个流程是否能成功运行，以及最终产出的数据是否符合预期。
- **大规模数据测试:** 使用一个1TB的ROS Bag数据集，运行一个转换流水线，测试系统的可伸缩性和性能。

------



## 🚀 7. 开发任务 (Getting Started)





#### **第一阶段：核心引擎与手动执行 (Core Engine & Manual Execution)**



- **任务1：搭建工作流引擎**
  - **交付物:** 在Kubernetes集群上成功部署Argo Workflows或Tekton。
- **任务2：开发第一个“算子”**
  - **交付物:** 一个`rosbag-reader`算子容器，它能读取一个ROS Bag文件，并将里面的图像消息提取为单独的JPG文件。
- **任务3：手动编写并运行第一个流水线**
  - **交付物:** 一个简单的`pipeline.yaml`，可以通过`argo submit`命令手动提交，并成功执行任务2中的算子。



#### **第二阶段：服务化与API (Service & API)**



- **任务1：开发`转换任务管理器`原型**
  - **交付物:** 一个Go服务，能接收gRPC请求，并将请求中的YAML内容动态地翻译成Argo/Tekton的CRD，然后提交到K8s集群。
- **任务2：实现与`数据中心`的集成**
  - **交付物:** 开发`s3-downloader`和`s3-uploader`两个标准算子，允许流水线从`数据中心`拉取数据并将结果上传回去。
- **任务3：提供`arf-cli`支持**
  - **交付物:** 开发者可以通过`arf-cli data convert --pipeline my_pipeline.yaml`来提交数据转换任务。



#### **第三阶段：生态与易用性 (Ecosystem & Usability)**



- **任务1：开发标准算子库**
  - **交付物:** 提供至少5个常用的、经过充分测试的官方算子（如图像解码、数据增强、时间戳对齐等）。
- **任务2：开发自定义算子SDK**
  - **交付物:** 一个Python库和项目模板，极大地简化开发者创建自定义算子的过程。
- **任务3：编写流水线教程**
  - **交付物:** 一份详细的教程，指导开发者如何编写`pipeline.yaml`，以及如何使用官方和自定义的算子来处理他们自己的数据集。



#### **第四阶段：大规模处理与优化 (Large-scale & Optimization)**



- **任务1：集成分布式处理框架**
  - **交付物:** 提供基于Spark或Dask的算子，用于处理单机无法处理的大规模数据集。
- **任务2：性能与成本优化**
  - **交付物:** 支持在流水线中使用K8s的Spot实例，以降低大规模数据处理的成本。
- **任务3：与`训练模块`联动**
  - **交付物:** 实现`ConverterService`在成功完成一个数据集的转换后，能够自动触发`训练模块`对这个新数据集进行模型训练的流程。

