# 🔥 ARF 模块开发参考文档：数据中心 (Data Hub)

> 🎯 **角色定位:** ARF的"中央数据湖"与"记忆核心" - 统一存储、管理和查询所有数据资产。
>
> 📦 **模块代号:** `arf-cloud-datahub`
>
> ⚡ **所属:** ARF 云端平面 (Cloud Plane)

---

## 📋 1. 核心职责与设计理念

### 🎯 核心使命 (Core Mission)

作为ARF生态系统的“中央数据湖”和“记忆核心”，数据中心的核心使命是**安全、可靠、高效地统一收集、存储、管理和查询**所有来自真实世界机器人集群和大规模仿真环境的数据。它是连接数据生产端（机器人、仿真）和数据消费端（训练、评测）的唯一枢纽。

主要应用场景包括：

- **💾 海量数据存储:** 为PB级的多模态机器人数据（图像、点云、轨迹、日志）提供一个统一的、可弹性伸缩的存储后端。
- **📚 数据资产管理:** 对所有数据集进行版本化、元数据管理和权限控制，确保数据的可追溯性和安全性。
- **🔍 高效数据检索:** 为`训练模块`和`评测模块`提供高效、灵活的数据查询接口，让它们可以像查询数据库一样获取所需的数据子集。
- **🌱 数据生态基础:** 作为整个ARF数据驱动开发模式（Data-Driven Development）的基石。

### 🏗️ 核心架构：湖仓一体 (Lakehouse Architecture)

我们采用现代的“湖仓一体”架构，兼顾数据湖的灵活性和数据仓库的高性能查询能力。

- **存储层 (Storage Layer):** 基于**对象存储**（如S3/MinIO），以开放的数据格式（如Parquet）存储所有原始和处理后的数据。这提供了极致的可伸缩性和成本效益。
- **元数据层 (Metadata Layer):** 使用表格格式（如Apache Iceberg）来管理存储在对象存储上的数据文件，为其提供ACID事务、模式演进和时间旅行（版本回溯）等“数据库”级别的能力。
- **计算层 (Compute Layer):** 数据查询和处理由`训练模块`、`数据转换模块`等外部的计算引擎（如Spark, Presto）来完成，实现了**存算分离**。

### ⚖️ 设计原则 (Design Principles)

- **📖 开放格式优先 (Open Formats First):** 所有数据都必须以开放的、社区标准的格式（Parquet, Arrow, Protobuf）存储，避免厂商锁定。
- **🔒 安全与合规:** 必须提供精细的访问控制（RBAC）和数据加密，确保数据安全，并支持GDPR等隐私合规要求。
- **🚀 存算分离 (Separation of Storage and Compute):** 存储层和计算层独立扩展，提供最大的灵活性和效率。
- **🔄 版本化一切 (Version Everything):** 所有的数据集和模型都必须是可版本化的，确保任何实验的可复现性。

---

## 📝 2. 核心需求 (Core Requirements)

| ID      | 需求描述                    | 验收标准                                                     | 优先级   |
| :------ | :-------------------------- | :----------------------------------------------------------- | :------- |
| **DH1** | **海量数据存储与管理**      | 系统必须能存储和管理超过1 PB的数据，并支持数据集的版本控制。 | **最高** |
| **DH2** | **高吞吐数据摄入**          | 必须提供一个高吞吐量的gRPC/S3接口，支持从数百个边缘设备同时上传数据。 | **最高** |
| **DH3** | **结构化数据查询**          | 必须提供一个类SQL的查询接口，允许用户根据元数据（如日期、机器人ID、任务类型）高效地检索数据。 | **最高** |
| **DH4** | **与数据转换/训练模块集成** | `数据转换`和`训练`模块必须能无缝地从`数据中心`读取源数据并将结果写回。 | **高**   |
| **DH5** | **精细化访问控制**          | 必须实现基于角色的访问控制(RBAC)，可以限制不同用户/服务对特定数据集的读写权限。 | **高**   |
| **DH6** | **数据生命周期管理**        | 必须支持自动化的数据生命周期策略（如自动归档或删除超过一年的旧数据）。 | **中**   |

---

## ⚙️ 3. 关键功能与子模块架构

### 🧠 3.1 数据摄入网关 (Data Ingestion Gateway)

**大门角色**：这是一个用`Go`实现的、高并发的gRPC服务，是所有数据进入`数据中心`的唯一入口。

- **协议适配:** 支持通过gRPC流式上传，也支持生成S3预签名URL让客户端直接上传大文件。
- **初步校验与元数据提取:** 对上传的数据包进行完整性校验，并提取元数据写入元数据层。
- **写入队列:** 将待处理的数据写入一个消息队列（如Kafka），由后续的处理服务进行消费。

### 📚 3.2 核心存储与元数据层 (Storage & Metadata Core)

**图书馆角色**：这是数据中心的基石。

- **对象存储 (Object Storage):** 使用MinIO或AWS S3作为底层存储，提供无限的扩展能力。
- **表格格式 (Table Format):** 采用**Apache Iceberg**作为元数据层。它为存储在S3上的数据文件提供了强大的事务和管理能力。
- **数据目录 (Data Catalog):** (可选)集成一个数据目录服务（如Amundsen），为用户提供一个可视化的界面来发现和理解数据集。

### 🔍 3.3 数据查询引擎 (Data Query Engine)

**检索工具角色**：我们不自建查询引擎，而是集成业界成熟的分布式查询引擎。

- **查询接口服务:** 提供一个gRPC服务，接收类SQL的查询请求。
- **查询引擎集成:** 该服务将查询请求翻译并提交给**Presto**或**Spark SQL**等分布式查询引擎来执行。查询引擎会直接读取存储在S3上的Parquet文件，并返回结果。

---

## 🔗 4. 接口设计与数据流

`数据中心`是云端所有数据操作的中心。

### 📥 输入数据流

| **数据源**       | **数据类型**          | **优先级** | **延迟要求** | **示例场景**               |
| :--------------- | :-------------------- | :--------- | :----------- | :------------------------- |
| **边缘端DMS**    | 数据文件包 (.parquet) | `NORMAL`   | N/A (异步)   | 机器人上传一天的数据记录   |
| **仿真模块**     | 仿真日志/传感器数据   | `NORMAL`   | N/A (异步)   | 大规模仿真结束后上传数据   |
| **数据转换模块** | 处理后的数据集        | `HIGH`     | 高带宽       | 将清洗后的数据写回数据中心 |

### 📤 输出数据流

| **目标模块**      | **数据类型**        | **保证**   | **性能指标**            |
| :---------------- | :------------------ | :--------- | :---------------------- |
| **训练模块**      | 批量的训练数据      | 高带宽     | TB级数据读取 < 1小时    |
| **评测模块**      | 标准化的评测数据集  | 可靠读取   | -                       |
| **开发者/分析师** | 查询结果 (CSV/JSON) | 交互式响应 | Peta-byte级数据秒级响应 |

### 🧬 核心API草案 (`data_hub.proto`)

```protobuf
// protos/arf/cloud/v1/data_hub.proto
syntax = "proto3";

package arf.cloud.v1;

// 数据中心服务
service DataHubService {
  // --- 写入 ---
  // 获取一个用于直接上传大文件的预签名URL
  rpc GetUploadUrl(GetUploadUrlRequest) returns (GetUploadUrlResponse);
  // 通知数据中心上传已完成，并触发元数据更新
  rpc NotifyUploadComplete(NotifyUploadCompleteRequest) returns (NotifyUploadCompleteResponse);

  // --- 读取 ---
  // 执行一个查询任务，并返回一个job_id
  rpc ExecuteQuery(ExecuteQueryRequest) returns (ExecuteQueryResponse);
  // 根据job_id获取查询结果
  rpc GetQueryResult(GetQueryResultRequest) returns (stream QueryResult);
}

message GetUploadUrlRequest {
    string dataset_name = 1;
    string file_name = 2;
}
message GetUploadUrlResponse {
    string upload_url = 1;
}
// ... 其他消息定义 ...
message ExecuteQueryRequest {
    string sql_query = 1;
}
message ExecuteQueryResponse {
    string job_id = 1;
}
message QueryResult {
    // 使用Apache Arrow的二进制格式进行流式传输，以获得最高性能
    bytes arrow_batch = 1;
}

// ... 其他请求和响应消息定义 ...
```

------



## 🛠️ 5. 技术栈与开发环境





### 💻 核心技术栈



| **技术领域**     | **选型**                         | **版本要求**               | **用途说明**                            |
| ---------------- | -------------------------------- | -------------------------- | --------------------------------------- |
| **核心服务语言** | **Go / Python**                  | `Go 1.21+`, `Python 3.10+` | Go用于高性能网关，Python用于数据处理API |
| **对象存储**     | **MinIO / AWS S3**               | 最新稳定版                 | 底层数据湖存储                          |
| **表格格式**     | **Apache Iceberg**               | 最新稳定版                 | 提供数据仓库能力                        |
| **查询引擎**     | **Trino (PrestoDB) / Spark SQL** | 最新稳定版                 | 执行分布式SQL查询                       |
| **测试框架**     | **Go Native Test, Pytest**       | 最新稳定版                 | 单元测试和接口测试                      |

------



## 🔧 6. 开发实施细节





### 🏗️ 6.1 项目结构 V1



```
services/cloud-plane/data-hub/
├── ingestion-gateway/       # 数据摄入网关 (Go)
│   ├── internal/
│   └── server/
├── query-service/           # 数据查询服务 (Python/Go)
│   └── ...
├── api/                     # .proto文件的软链接
├── deployments/
│   └── helm/                # 用于部署整个数据湖仓的Helm Chart
└── README.md
```



### 🧪 6.2 测试与验证策略



- **单元测试:** 对查询解析、元数据管理等核心逻辑进行单元测试。
- **集成测试:**
  - **端到端摄入测试:** 编写脚本，模拟边缘端DMS上传一个1GB的数据包，验证数据能否被正确摄入、转换并出现在Iceberg表中。
  - **查询正确性测试:** 摄入一个已知的数据集，然后执行一系列SQL查询，验证返回结果是否与预期完全一致。
  - **性能基准测试:** 使用TPC-H等标准数据集，测试查询引擎在不同数据规模下的性能表现。

------



## 🚀 7. 开发任务 (Getting Started)





#### **第一阶段：基础存储与手动上传 (Basic Storage & Manual Upload)**



- **任务1：部署对象存储和元数据层**
  - **交付物:** 一个在Kubernetes上运行的MinIO集群和一个配置好的Iceberg Catalog（可以使用Nessie或JDBC）。
- **任务2：开发数据摄入网关v0.1**
  - **交付物:** 一个Go服务，能提供S3预签名URL，允许用户通过`arf-cli`或`curl`手动上传文件。
- **任务3：手动数据加载与查询**
  - **交付物:** 提供一个Jupyter Notebook教程，指导开发者如何使用Spark手动将上传的Parquet文件加载为Iceberg表，并执行简单的SQL查询。



#### **第二阶段：服务化查询与自动化摄入 (Query Service & Automated Ingestion)**



- **任务1：开发数据查询服务**
  - **交付物:** 一个gRPC服务，能接收SQL查询，并将其代理到Trino/Spark SQL引擎执行。
- **任务2：自动化数据摄入**
  - **交付物:** 摄入网关在文件上传完成后，能自动触发一个`数据转换模块`的流水线，将数据自动转换为Iceberg表。
- **任务3：提供`arf-cli`支持**
  - **交付物:** `arf-cli data upload`和`arf-cli data query "SELECT * ..."`命令能正常工作。



#### **第三阶段：权限与版本管理 (Access Control & Versioning)**



- **任务1：实现RBAC访问控制**
  - **交付物:** 实现基于JWT或OAuth2的认证，并集成到查询服务中，确保用户只能查询他们有权限访问的数据集。
- **任务2：实现数据集版本化**
  - **交付物:** 利用Iceberg的时间旅行能力，提供API允许用户查询一个数据集的历史版本。
- **任务3：开发数据目录原型**
  - **交付物:** 部署一个Amundsen等开源数据目录，并将其与我们的Iceberg表集成，提供数据发现的UI。



#### **第四阶段：性能优化与生命周期管理 (Performance & Lifecycle)**



- **任务1：查询性能优化**
  - **交付物:** 对Iceberg表进行分区(Partitioning)和压缩(Compaction)等优化，提升热点数据的查询性能。
- **任务2：实现数据生命周期管理**
  - **交付物:** 实现自动化策略，将超过90天未被访问的数据自动归档到更低成本的存储层（如S3 Glacier）。
- **任务3：压力测试与成本分析**
  - **交付物:** 提交一份测试报告，展示`数据中心`在TB/PB级数据规模下的性能表现和存储成本。

